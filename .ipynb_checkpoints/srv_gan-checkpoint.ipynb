{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# When running the model with conv2d\n",
    "# UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
    "# it is because the cnDNN version you installed is not compatible with the cuDNN version that compiled in tensorflow. -> Let conda or pip automatically choose the right version of tensorflow and cudnn.\n",
    "# or run out of graphics card RAM -> must set limit for GPU RAM. Splitting into 2 logical GPU with different RAM limit. By default, Tensorflow will use on the logical GPU: 0, the GPU: 1 will be used for training generator and discriminator models.\n",
    "\n",
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "# https://leimao.github.io/blog/TensorFlow-cuDNN-Failure/\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [\n",
    "         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*0.15),\n",
    "         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*5.45) # for Training\n",
    "#          tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*5) # for Testing\n",
    "        ])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_15fps_dir = os.path.join(*['data', 'REDS_VTSR', 'train', 'train_15fps'])\n",
    "# print(train_15fps_dir)\n",
    "\n",
    "train_30fps_dir = os.path.join(*['data', 'REDS_VTSR', 'train', 'train_30fps'])\n",
    "# print(train_30fps_dir)\n",
    "\n",
    "train_60fps_dir = os.path.join(*['data', 'REDS_VTSR', 'train', 'train_60fps'])\n",
    "# print(train_60fps_dir)\n",
    "\n",
    "val_15fps_dir = os.path.join(*['data', 'REDS_VTSR', 'val', 'val_15fps'])\n",
    "# print(val_15fps_dir)\n",
    "\n",
    "val_30fps_dir = os.path.join(*['data', 'REDS_VTSR', 'val', 'val_30fps'])\n",
    "# print(val_30fps_dir)\n",
    "\n",
    "val_60fps_dir = os.path.join(*['data', 'REDS_VTSR', 'val', 'val_60fps'])\n",
    "# print(val_60fps_dir)\n",
    "\n",
    "test_15fps_dir = os.path.join(*['data', 'REDS_VTSR', 'test', 'test_15fps'])\n",
    "# print(test_15fps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_15fps_dir = [os.path.join(train_15fps_dir, p) for p in os.listdir(train_15fps_dir)]\n",
    "# print('Train 15fps', train_15fps_dir[:2])\n",
    "\n",
    "train_30fps_dir = [os.path.join(train_30fps_dir, p) for p in os.listdir(train_30fps_dir)]\n",
    "# print('Train 30fps', train_30fps_dir[:2])\n",
    "\n",
    "train_60fps_dir = [os.path.join(train_60fps_dir, p) for p in os.listdir(train_60fps_dir)]\n",
    "# print('Train 60fps', train_60fps_dir[:2])\n",
    "\n",
    "val_15fps_dir = [os.path.join(val_15fps_dir, p) for p in os.listdir(val_15fps_dir)]\n",
    "# print('Val 15fps', val_15fps_dir[:2])\n",
    "\n",
    "val_30fps_dir = [os.path.join(val_30fps_dir, p) for p in os.listdir(val_30fps_dir)]\n",
    "# print('Val 30fps', val_30fps_dir[:2])\n",
    "\n",
    "val_60fps_dir = [os.path.join(val_60fps_dir, p) for p in os.listdir(val_60fps_dir)]\n",
    "# print('Val 60fps', val_60fps_dir[:2])\n",
    "\n",
    "test_15fps_dir = [os.path.join(test_15fps_dir, p) for p in os.listdir(test_15fps_dir)]\n",
    "# print('Val 15fps', test_15fps_dir[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print([len(os.listdir(i)) for i in train_30fps_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. None Images\n",
    "in 30 fps and 60 fps folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(os.path.join(train_30fps_dir[0], '00000012.png'))\n",
    "# print(image)\n",
    "# print(os.path.getsize(os.path.join(train_30fps_dir[0], '00000012.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Delete None Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run one.\n",
    "# count = 0\n",
    "\n",
    "# for folder in [train_30fps_dir, train_60fps_dir, val_30fps_dir, val_60fps_dir]:\n",
    "#     for paths in folder:\n",
    "#         paths = pathlib.Path(paths)\n",
    "\n",
    "#         for p in paths.glob('*'):\n",
    "\n",
    "#             if cv2.imread(str(p)) is None:\n",
    "#                 os.remove(str(p))\n",
    "#                 count+=1\n",
    "# print(f'Deleted {count} files')\n",
    "# Deleted 35809 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Randomize Videos Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [train_30fps_dir, train_60fps_dir, val_30fps_dir, val_60fps_dir]\n",
    "import random\n",
    "random.shuffle(train_30fps_dir) # make the training dataset random\n",
    "random.shuffle(train_60fps_dir) # make the training dataset random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Get Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: all frames of 1 video as 1 element.\n",
    "# Failed due to unable to flatten tensor objects\n",
    "\n",
    "# image_30fps_paths = []\n",
    "# for video_path in train_30fps_dir:\n",
    "#     image_30fps_paths.append([os.path.join(video_path, x) for x in os.listdir(video_path)])\n",
    "\n",
    "# output format: [[image1.png, image2.png,...], [image1.png, image2.png,...],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_30fps_paths = []\n",
    "for video_path in train_30fps_dir:\n",
    "    for x in os.listdir(video_path):\n",
    "        train_image_30fps_paths.append(os.path.join(video_path, x))\n",
    "\n",
    "# output format: [image1.png, image2.png,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_30fps_paths = []\n",
    "for video_path in val_30fps_dir:\n",
    "    for x in os.listdir(video_path):\n",
    "        val_image_30fps_paths.append(os.path.join(video_path, x))\n",
    "\n",
    "# output format: [image1.png, image2.png,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_15fps_paths = []\n",
    "for video_path in test_15fps_dir:\n",
    "    for x in os.listdir(video_path):\n",
    "        test_image_15fps_paths.append(os.path.join(video_path, x))\n",
    "\n",
    "# output format: [image1.png, image2.png,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Train Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# # test: all frames of 1 video as 1 element.\n",
    "# # Failed due to unable to flatten tensor objects\n",
    "\n",
    "# def reverse(image_paths):\n",
    "#     \"\"\"\n",
    "#     Function that randomly reverse frames sequence in 1 video.\n",
    "#     1 video can only have 2 states: reversed or not reversed.\n",
    "#     Args:\n",
    "#         image_paths: Tensor object, list, tuple. The list of paths to frames in the video.\n",
    "#     Returns:\n",
    "#         image_paths: The list of paths to frames in the video with reversed order.\n",
    "#     \"\"\"\n",
    "#     method_list = ['reverse', None]\n",
    "#     reverse_method = random.choice(method_list)\n",
    "    \n",
    "#     if reverse_method == 'reverse':\n",
    "#         image_paths = tf.reverse(image_paths, axis=[0])\n",
    "        \n",
    "#     return image_paths\n",
    "\n",
    "# def flip_method(images):\n",
    "#     \"\"\"\n",
    "#     Function that flip horizontally/vertically all frames in 1 video.\n",
    "#     Args:\n",
    "#         images: Tensor object, list, tuple. The list of frames in the video.\n",
    "#     Returns:\n",
    "#         images: Tensor object, list, tuple. The list of (unchanged) frames in the video.\n",
    "#         method: flip or not\n",
    "#     \"\"\" \n",
    "# #     flip the image randomly\n",
    "#     method_list = ['horizontal', 'vertical', None]\n",
    "#     flip_method = random.choice(method_list)\n",
    "#     flip_method = np.array([flip_method] * 45)\n",
    "#     images = tf.squeeze(images)\n",
    "    \n",
    "#     return images, flip_method\n",
    "\n",
    "# image = tf.io.read_file(i)\n",
    "#         image = tf.image.decode_jpeg(image, channels=3)\n",
    "#         image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "# def parse_image(image_paths):\n",
    "#     \"\"\"\n",
    "#     Function that loads the images given the path.\n",
    "#     Args:\n",
    "#         image_paths: Tensor object, list, tuple. The list of paths to frames in the video.\n",
    "#     Returns:\n",
    "#         image_paths: A tf tensor of the loaded frames.\n",
    "#     \"\"\"\n",
    "#     parsed_images = []\n",
    "#     print(image_paths)\n",
    "#     for i in image_paths[0]:\n",
    "#         print(i)\n",
    "#         image = tf.io.read_file(i)\n",
    "#         image = tf.image.decode_jpeg(image, channels=3)\n",
    "#         image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#         print(image)\n",
    "#         try:\n",
    "#             parsed_images = tf.stack([parsed_images, image])\n",
    "#         except:\n",
    "#             parsed_images = image\n",
    "# #         parsed_images.append(image)\n",
    "\n",
    "#     return [parsed_images]\n",
    "\n",
    "\n",
    "\n",
    "# def high_low_res_pairs(high_res, scale=4):\n",
    "#     \"\"\"\n",
    "#     Function that generates a low resolution image given the \n",
    "#     high resolution image.\n",
    "#     Args:\n",
    "#         high_res: A tf tensor of the high res image.\n",
    "#         scale: Int, The downsampling factor, default is 4x.\n",
    "#     Returns:\n",
    "#         low_res: A tf tensor of the low res image.\n",
    "#         high_res: A tf tensor of the high res image.\n",
    "#     \"\"\"\n",
    "#     method_list = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\n",
    "#     downsampling_method = random.choice(method_list)\n",
    "    \n",
    "#     low_res = tf.image.resize(high_res, \n",
    "#                               [high_res.shape[0] // scale, high_res.shape[1] // scale],\n",
    "#                               preserve_aspect_ratio=True,\n",
    "#                               method=downsampling_method)    \n",
    "#     return low_res, high_res\n",
    "\n",
    "# def rescale(low_res, high_res):\n",
    "#     \"\"\"\n",
    "#     Function that rescales the pixel values of high_res to the -1 to 1 range.\n",
    "#     For use with the generator output tanh function.\n",
    "#     Args:\n",
    "#         low_res: The tf tensor of the low res image.\n",
    "#         high_res: The tf tensor of the high res image.\n",
    "#     Returns:\n",
    "#         low_res: The tf tensor of the low res image, rescaled.\n",
    "#         high_res: The tf tensor of the high res image, rescaled.\n",
    "#     \"\"\"\n",
    "#     high_res = high_res * 2.0 - 1.0\n",
    "\n",
    "#     return low_res, high_res\n",
    "\n",
    "# def parse_video(video_path, image_paths):\n",
    "#     \"\"\"\n",
    "#     Function that loads frames of the video in given path.\n",
    "#     Args:\n",
    "#         video_path: Tensor object that contains path to a video folder.\n",
    "#     Predefined functions:\n",
    "#         parse_image\n",
    "#         high_low_res_pairs\n",
    "#         rescale\n",
    "#         reverse\n",
    "#         flip\n",
    "#     Returns:\n",
    "#         low_res: A tf tensor of the low res image.\n",
    "#         high_res: A tf tensor of the high res image.\n",
    "#     \"\"\"\n",
    "#     low_res_images = []\n",
    "#     high_res_images = []\n",
    "    \n",
    "#     print(video_path)\n",
    "#     print(image_paths)\n",
    "# #     print(tf.convert_to_tensor(video_path))\n",
    "# #     return\n",
    "# #     video_path format: Tensor(\"args_0:0\", shape=(1,), dtype=string)\n",
    "# # -> it makes: image_paths = [os.path.join(video_path, x) for x in os.listdir(video_path)] get error\n",
    "\n",
    "# #     image_paths = [os.path.join(video_path, x) for x in os.listdir(video_path)]\n",
    "    \n",
    "#     for i in range(len(image_paths)):\n",
    "#         image_paths[i] = reverse(image_paths[i])\n",
    "    \n",
    "#         for path_to_image in image_paths:\n",
    "\n",
    "#     #         read image\n",
    "#             image = parse_image(path_to_image)\n",
    "\n",
    "\n",
    "\n",
    "#             image = flip(image, method=flip_method)\n",
    "\n",
    "#     #         low/high res images\n",
    "#             low_res, high_res = high_low_res_pairs(image, scale=4)\n",
    "#             low_res, high_res = rescale(low_res, high_res)\n",
    "\n",
    "#             low_res_images.append(low_res)\n",
    "#             high_res_images.append(high_res)\n",
    "    \n",
    "#     return np.array(low_res_images), np.array(high_res_images)\n",
    "# #     sample output: [(np.array[low_res_images], np.array[high_res_images]),\n",
    "# #                     (np.array[low_res_images], np.array[high_res_images]),\n",
    "# #                     ...]\n",
    "\n",
    "# # result must be list with shape (1,)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_height = 360\n",
    "hr_width = 640\n",
    "scale = 4\n",
    "\n",
    "lr_height = hr_height // scale\n",
    "lr_width = hr_width // scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 image as 1 element\n",
    "\n",
    "def reverse(ds):\n",
    "    \"\"\"\n",
    "    Function that randomly reverse frames sequence in 1 video.\n",
    "    Args:\n",
    "        ds: A tf dataset.\n",
    "    Returns:\n",
    "        ds: A tf dataset with reversed frames sequence.\n",
    "    \"\"\" \n",
    "#     reverse squence randomly\n",
    "    method_list = ['reverse', None]\n",
    "    reverse_method = random.choice(method_list)\n",
    "    \n",
    "    image_list = list(ds.as_numpy_iterator())\n",
    "\n",
    "    if reverse_method == 'reverse':\n",
    "        image_list.reverse()\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(image_list)\n",
    "\n",
    "def parse_image(image_path):\n",
    "    \"\"\"\n",
    "    Function that loads the images given the path.\n",
    "    Args:\n",
    "        image_path: The paths to frames in the video.\n",
    "    Returns:\n",
    "        image: A tf tensor of the loaded frames.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    return image\n",
    "\n",
    "def flip(ds):\n",
    "    \"\"\"\n",
    "    Function that flip horizontally/vertically all images in 1 dataset.\n",
    "    Args:\n",
    "        ds: A tf dataset.\n",
    "    Returns:\n",
    "        ds: A tf dataset with flipped images.\n",
    "    \"\"\" \n",
    "#     flip the image randomly\n",
    "    method_list = ['horizontal', 'vertical', None]\n",
    "    flip_method = random.choice(method_list)\n",
    "    \n",
    "    def flip_left_right(image):\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        return image\n",
    "    \n",
    "    def flip_up_down(image):\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        return image\n",
    "    \n",
    "    if flip_method == 'horizontal':\n",
    "        ds = ds.map(flip_up_down, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    elif flip_method == 'vertical':\n",
    "        ds = ds.map(flip_left_right, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "def high_low_res_pairs(ds):\n",
    "    \"\"\"\n",
    "    Function that generates a low resolution image given the high resolution image with random methods.\n",
    "    Listed methods: ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\n",
    "    Default downsampling factor is 4x.\n",
    "    Args:\n",
    "        ds: A tf dataset.\n",
    "    Returns:\n",
    "        ds: A tf dataset with low and high res images.\n",
    "    \"\"\"\n",
    "    method_list = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\n",
    "    downsampling_method = random.choice(method_list)\n",
    "    \n",
    "    def downsampling(high_res):\n",
    "        \"\"\"\n",
    "        Function that generates a low resolution image given the high resolution image.\n",
    "        Args:\n",
    "            high_res: A tf tensor of the high res image.\n",
    "        Returns:\n",
    "            low_res: A tf tensor of the low res image.\n",
    "            high_res: A tf tensor of the high res image.\n",
    "        \"\"\"\n",
    "#         print(tf.shape(high_res)[0])\n",
    "        low_res = tf.image.resize(high_res, \n",
    "                                  [lr_height, lr_width],\n",
    "                                  preserve_aspect_ratio=True,\n",
    "                                  method=downsampling_method)\n",
    "    \n",
    "        high_res = tf.image.resize(high_res, \n",
    "                                  [hr_height, hr_width],\n",
    "                                  preserve_aspect_ratio=True,\n",
    "                                  method=downsampling_method)\n",
    "        return low_res, high_res\n",
    "    \n",
    "    ds = ds.map(downsampling, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def rescale(low_res, high_res):\n",
    "    \"\"\"\n",
    "    Function that rescales the pixel values of high_res to the -1 to 1 range.\n",
    "    For use with the generator output tanh function.\n",
    "    Args:\n",
    "        low_res: The tf tensor of the low res image.\n",
    "        high_res: The tf tensor of the high res image.\n",
    "    Returns:\n",
    "        low_res: The tf tensor of the low res image, rescaled.\n",
    "        high_res: The tf tensor of the high res image, rescaled.\n",
    "    \"\"\"\n",
    "    high_res = high_res * 2.0 - 1.0\n",
    "\n",
    "    return low_res, high_res\n",
    "\n",
    "def dataset(image_paths, batch_size=2):\n",
    "    \"\"\"\n",
    "    Returns a tf dataset object with specified mappings. No shuffle and No repeat.\n",
    "    No shuffle because it will screw up the frame sequence.\n",
    "    No repeat because training model will use a manual for loop.\n",
    "    Args:\n",
    "        image_paths: Str, Path to images.\n",
    "        batch_size: Int, The number of elements in a batch returned by the dataset.\n",
    "    Returns:\n",
    "        dataset: A tf dataset object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate tf dataset from high res video paths.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "\n",
    "    # Prefetch the data for optimal GPU utilization.\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # apply: Applies a transformation function to the whole dataset as once. Good for functions with the same random arg.\n",
    "\n",
    "    # randomly reverse frames sequence in 1 video\n",
    "    dataset = dataset.apply(reverse)\n",
    "\n",
    "    # image paths to tensor\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # randomly flip all frames in 1 video\n",
    "    dataset = dataset.apply(flip)\n",
    "\n",
    "    # Generate low resolution by downsampling.\n",
    "    dataset = dataset.apply(high_low_res_pairs)\n",
    "\n",
    "    # Rescale the values in the input\n",
    "    dataset = dataset.map(rescale, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Batch the input, drop remainder to get a defined batch size.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image_test = parse_image(os.path.join(train_30fps_dir[0], '00000012.png'))\n",
    "# low, high = high_low_res_pairs(image_test, scale=4)\n",
    "\n",
    "# f, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "# ax1.imshow(low)\n",
    "# ax1.set_title('Low Res')\n",
    "\n",
    "# ax2.imshow(high)\n",
    "# ax2.set_title('High Res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_image_30fps_paths, batch_size=2)\n",
    "# sample_train_dataset = dataset(train_image_30fps_paths[:180], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tf.io.read_file(dataset.take(1)[0][0][0]))\n",
    "\n",
    "# f, ax = plt.subplots(4, 2, figsize=(40, 50))\n",
    "# i=0\n",
    "# for f in dataset.take(2):\n",
    "#     for c in f[:2]:\n",
    "#         ax[i][0].imshow(tf.squeeze(c[0]))\n",
    "#         ax[i][1].imshow(tf.squeeze(c[1]))\n",
    "\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Validation + Test Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 image as 1 element\n",
    "def val_low_res(ds):\n",
    "    \"\"\"\n",
    "    Function that generates a low resolution image given the high resolution image with random methods.\n",
    "    Listed methods: ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\n",
    "    Default downsampling factor is 4x.\n",
    "    Args:\n",
    "        ds: A tf dataset.\n",
    "    Returns:\n",
    "        ds: A tf dataset with low and high res images.\n",
    "    \"\"\"\n",
    "    method_list = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\n",
    "    downsampling_method = random.choice(method_list)\n",
    "    \n",
    "    def downsampling(high_res):\n",
    "        \"\"\"\n",
    "        Function that generates a low resolution image given the high resolution image.\n",
    "        Args:\n",
    "            high_res: A tf tensor of the high res image.\n",
    "        Returns:\n",
    "            low_res: A tf tensor of the low res image.\n",
    "            high_res: A tf tensor of the high res image.\n",
    "        \"\"\"\n",
    "#         print(tf.shape(high_res)[0])\n",
    "        low_res = tf.image.resize(high_res, \n",
    "                                  [lr_height, lr_width],\n",
    "                                  preserve_aspect_ratio=True,\n",
    "                                  method=downsampling_method)   \n",
    "        return low_res\n",
    "    \n",
    "    ds = ds.map(downsampling, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def val_dataset(image_paths, batch_size=2):\n",
    "    \"\"\"\n",
    "    Returns a tf dataset object with specified mappings. No shuffle and repeat.\n",
    "    Args:\n",
    "        image_paths: Str, Path to images.\n",
    "        batch_size: Int, The number of elements in a batch returned by the dataset.\n",
    "    Returns:\n",
    "        dataset: A tf dataset object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate tf dataset from high res video paths.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "\n",
    "    # Prefetch the data for optimal GPU utilization.\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # image paths to tensor\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # Generate low resolution by downsampling.\n",
    "    dataset = dataset.apply(val_low_res)\n",
    "\n",
    "    # Batch the input, drop remainder to get a defined batch size.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset(val_image_30fps_paths, batch_size=2)\n",
    "# sample_val_dataset = val_dataset(val_image_30fps_paths[:90], batch_size=2)\n",
    "\n",
    "#  test_dataset = val_dataset(test_image_15fps_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_shape = (hr_height, hr_width, 3)\n",
    "lr_shape = (lr_height, lr_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 360, 640, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 360, 640, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 360, 640, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 180, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 180, 320, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 180, 320, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 90, 160, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 90, 160, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 90, 160, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 90, 160, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 90, 160, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 45, 80, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 45, 80, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 45, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 45, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 45, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 22, 40, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 22, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 22, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 22, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 22, 40, 512)       2359808   \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "# and the generated high resolution images and minimize the mse between them\n",
    "# Get the vgg network. Extract features from Block 5, last convolution, exclude layer block5_pool (MaxPooling2D)\n",
    "vgg = tf.keras.applications.VGG19(weights=\"imagenet\", input_shape=hr_shape, include_top=False)\n",
    "vgg.trainable = False\n",
    "\n",
    "# Create model and compile\n",
    "vgg_model = tf.keras.models.Model(inputs=vgg.input, outputs=vgg.get_layer(\"block5_conv4\").output)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def content_loss(hr, sr):\n",
    "    \"\"\"\n",
    "    Returns Mean Square Error of VGG19 feature extracted original image (y) and VGG19 feature extracted generated image (y_hat).\n",
    "    Args:\n",
    "        hr: A tf tensor of original image (y)\n",
    "        sr: A tf tensor of generated image (y_hat)\n",
    "    Returns:\n",
    "        mse: Mean Square Error.\n",
    "    \"\"\"\n",
    "    sr = tf.keras.applications.vgg19.preprocess_input(((sr + 1.0) * 255) / 2.0)\n",
    "    hr = tf.keras.applications.vgg19.preprocess_input(((hr + 1.0) * 255) / 2.0)\n",
    "    sr_features = vgg_model(sr) / 12.75\n",
    "    hr_features = vgg_model(hr) / 12.75\n",
    "    mse = tf.keras.losses.MeanSquaredError()(hr_features, sr_features)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \"\"\"Build the generator that will do the Super Resolution task.\n",
    "    Based on the Mobilenet design. Idea from Galteri et al.\"\"\"\n",
    "\n",
    "    def make_divisible(v, divisor, min_value=None):\n",
    "            if min_value is None:\n",
    "                min_value = divisor\n",
    "            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "            # Make sure that round down does not go down by more than 10%.\n",
    "            if new_v < 0.9 * v:\n",
    "                new_v += divisor\n",
    "            return new_v\n",
    "\n",
    "    def residual_block(inputs, filters, block_id, expansion=6, stride=1, alpha=1.0):\n",
    "        \"\"\"Inverted Residual block that uses depth wise convolutions for parameter efficiency.\n",
    "        Args:\n",
    "            inputs: The input feature map.\n",
    "            filters: Number of filters in each convolution in the block.\n",
    "            block_id: An integer specifier for the id of the block in the graph.\n",
    "            expansion: Channel expansion factor.\n",
    "            stride: The stride of the convolution.\n",
    "            alpha: Depth expansion factor.\n",
    "        Returns:\n",
    "            x: The output of the inverted residual block.\n",
    "        \"\"\"\n",
    "        channel_axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "        in_channels = keras.backend.int_shape(inputs)[channel_axis]\n",
    "        pointwise_conv_filters = int(filters * alpha)\n",
    "        pointwise_filters = make_divisible(pointwise_conv_filters, 8)\n",
    "        x = inputs\n",
    "        prefix = 'block_{}_'.format(block_id)\n",
    "\n",
    "        if block_id:\n",
    "            # Expand\n",
    "            x = keras.layers.Conv2D(expansion * in_channels, kernel_size=1, padding='same', use_bias=True, activation=None,\n",
    "                                    name=prefix + 'expand')(x)\n",
    "            x = keras.layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999,\n",
    "                                                name=prefix + 'expand_BN')(x)\n",
    "            x = keras.layers.Activation('relu', name=prefix + 'expand_relu')(x)\n",
    "        else:\n",
    "            prefix = 'expanded_conv_'\n",
    "\n",
    "        # Depthwise\n",
    "        x = keras.layers.DepthwiseConv2D(kernel_size=3, strides=stride, activation=None, use_bias=True, padding='same' if stride == 1 else 'valid',\n",
    "                                         name=prefix + 'depthwise')(x)\n",
    "        x = keras.layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999,\n",
    "                                            name=prefix + 'depthwise_BN')(x)\n",
    "\n",
    "        x = keras.layers.Activation('relu', name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "        # Project\n",
    "        x = keras.layers.Conv2D(pointwise_filters, kernel_size=1, padding='same', use_bias=True, activation=None,\n",
    "                                name=prefix + 'project')(x)\n",
    "        x = keras.layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999,\n",
    "                                            name=prefix + 'project_BN')(x)\n",
    "\n",
    "        if in_channels == pointwise_filters and stride == 1:\n",
    "            return keras.layers.Add(name=prefix + 'add')([inputs, x])\n",
    "        return x\n",
    "\n",
    "    def deconv2d(layer_input):\n",
    "        \"\"\"Upsampling layer to increase height and width of the input.\n",
    "        Uses PixelShuffle for upsampling.\n",
    "        Args:\n",
    "            layer_input: The input tensor to upsample.\n",
    "        Returns:\n",
    "            u: Upsampled input by a factor of 2.\n",
    "        \"\"\"\n",
    "        \n",
    "        u = keras.layers.UpSampling2D(size=2, interpolation='bilinear')(layer_input)\n",
    "        \n",
    "        # Number of filters in the first layer. Realtime Image Enhancement GAN Galteri et al.\n",
    "        u = keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same')(u)\n",
    "        u = keras.layers.PReLU(shared_axes=[1, 2])(u)\n",
    "        return u\n",
    "\n",
    "    # Low resolution image input\n",
    "    img_lr = keras.Input(shape=lr_shape)\n",
    "\n",
    "    # Pre-residual block\n",
    "    c1 = keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same')(img_lr)\n",
    "    c1 = keras.layers.BatchNormalization()(c1)\n",
    "    c1 = keras.layers.PReLU(shared_axes=[1, 2])(c1)\n",
    "\n",
    "    # Propogate through residual blocks\n",
    "    r = residual_block(c1, 32, 0)\n",
    "    \n",
    "    # Number of inverted residual blocks in the mobilenet generator    \n",
    "    for idx in range(1, 6):\n",
    "        r = residual_block(r, 32, idx)\n",
    "\n",
    "    # Post-residual block\n",
    "    c2 = keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same')(r)\n",
    "    c2 = keras.layers.BatchNormalization()(c2)\n",
    "    c2 = keras.layers.Add()([c2, c1])\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(c2)\n",
    "    u2 = deconv2d(u1)\n",
    "\n",
    "    # Generate high resolution output\n",
    "    gen_hr = keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')(u2)\n",
    "\n",
    "    return keras.models.Model(img_lr, gen_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 90, 160, 3)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 90, 160, 32)  896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 90, 160, 32)  128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 90, 160, 32)  32          batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 90, 160, 32)  320         p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 90, 160, 32)  128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (A (None, 90, 160, 32)  0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 90, 160, 32)  1056        expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 90, 160, 32)  128         expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_add (Add)         (None, 90, 160, 32)  0           p_re_lu[0][0]                    \n",
      "                                                                 expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 90, 160, 192) 6336        expanded_conv_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 90, 160, 192) 768         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (Activation (None, 90, 160, 192) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 90, 160, 192) 1920        block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 90, 160, 192) 768         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (Activat (None, 90, 160, 192) 0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 90, 160, 32)  6176        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 90, 160, 32)  128         block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1_add (Add)               (None, 90, 160, 32)  0           expanded_conv_add[0][0]          \n",
      "                                                                 block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 90, 160, 192) 6336        block_1_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 90, 160, 192) 768         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (Activation (None, 90, 160, 192) 0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 90, 160, 192) 1920        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 90, 160, 192) 768         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (Activat (None, 90, 160, 192) 0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 90, 160, 32)  6176        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 90, 160, 32)  128         block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 90, 160, 32)  0           block_1_add[0][0]                \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 90, 160, 192) 6336        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 90, 160, 192) 768         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (Activation (None, 90, 160, 192) 0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 90, 160, 192) 1920        block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 90, 160, 192) 768         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (Activat (None, 90, 160, 192) 0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 90, 160, 32)  6176        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 90, 160, 32)  128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3_add (Add)               (None, 90, 160, 32)  0           block_2_add[0][0]                \n",
      "                                                                 block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 90, 160, 192) 6336        block_3_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 90, 160, 192) 768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (Activation (None, 90, 160, 192) 0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 90, 160, 192) 1920        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 90, 160, 192) 768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (Activat (None, 90, 160, 192) 0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 90, 160, 32)  6176        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 90, 160, 32)  128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 90, 160, 32)  0           block_3_add[0][0]                \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 90, 160, 192) 6336        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 90, 160, 192) 768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (Activation (None, 90, 160, 192) 0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 90, 160, 192) 1920        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 90, 160, 192) 768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (Activat (None, 90, 160, 192) 0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 90, 160, 32)  6176        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 90, 160, 32)  128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 90, 160, 32)  0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 90, 160, 32)  9248        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 90, 160, 32)  128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 90, 160, 32)  0           batch_normalization_1[0][0]      \n",
      "                                                                 p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 180, 320, 32) 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 180, 320, 32) 9248        up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 180, 320, 32) 32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 360, 640, 32) 0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 360, 640, 32) 9248        up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 360, 640, 32) 32          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 360, 640, 3)  867         p_re_lu_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 111,971\n",
      "Trainable params: 107,555\n",
      "Non-trainable params: 4,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use \n",
    "gen_model = build_generator()\n",
    "\n",
    "# # Generator loss\n",
    "# content_loss = content_loss(y, fake_hr)\n",
    "# adv_loss = 1e-3 * tf.keras.losses.BinaryCrossentropy()(valid, fake_prediction)\n",
    "# mse_loss = tf.keras.losses.MeanSquaredError()(y, fake_hr)\n",
    "# perceptual_loss = content_loss + adv_loss + mse_loss\n",
    "        \n",
    "# gen_model.compile(optimizer='Adam',\n",
    "#                   loss=tf.keras.losses.MeanSquaredError(),\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if gen_model can run or not\n",
    "\n",
    "# gen_predict_val = gen_model.predict(val_dataset)\n",
    "# gen_predict_val.shape\n",
    "# gen_predict_val = tf.cast(255*(gen_predict_val + 1)/2, tf.uint8)\n",
    "\n",
    "# UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
    "# it is because the cnDNN version you installed is not compatible with the cuDNN version that compiled in tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \"\"\"Builds a discriminator network based on the SRGAN design.\"\"\"\n",
    "\n",
    "    def d_block(layer_input, filters, strides=1, bn=True):\n",
    "        \"\"\"Discriminator layer block.\n",
    "        Args:\n",
    "            layer_input: Input feature map for the convolutional block.\n",
    "            filters: Number of filters in the convolution.\n",
    "            strides: The stride of the convolution.\n",
    "            bn: Whether to use batch norm or not.\n",
    "        \"\"\"\n",
    "        d = keras.layers.Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "        if bn:\n",
    "            d = keras.layers.BatchNormalization(momentum=0.8)(d)\n",
    "        d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "        return d\n",
    "\n",
    "    # Input img\n",
    "    d0 = keras.layers.Input(shape=hr_shape)\n",
    "\n",
    "    d1 = d_block(d0, 32, bn=False)\n",
    "    d2 = d_block(d1, 32, strides=2)\n",
    "    d3 = d_block(d2, 32)\n",
    "    d4 = d_block(d3, 32, strides=2)\n",
    "    d5 = d_block(d4, 64)\n",
    "    d6 = d_block(d5, 64, strides=2)\n",
    "    d7 = d_block(d6, 64)\n",
    "    d8 = d_block(d7, 64, strides=2)\n",
    "\n",
    "    validity = keras.layers.Conv2D(1, kernel_size=1, strides=1, activation='sigmoid', padding='same')(d8)\n",
    "\n",
    "    return keras.models.Model(d0, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 360, 640, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 360, 640, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 360, 640, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 180, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 180, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 180, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 180, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 180, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 180, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 90, 160, 32)       9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 90, 160, 32)       128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 90, 160, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 90, 160, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 90, 160, 64)       256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 90, 160, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 45, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 45, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 45, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 45, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 45, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 45, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 23, 40, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 23, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 23, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 23, 40, 1)         65        \n",
      "=================================================================\n",
      "Total params: 159,393\n",
      "Trainable params: 158,689\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = build_discriminator()\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate decay schedule.\n",
    "lr = 1e-4\n",
    "\n",
    "gen_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    lr,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.1,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "disc_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    lr * 5,  # TTUR - Two Time Scale Updates\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.1,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "gen_optimizer = keras.optimizers.Adam(learning_rate=gen_schedule)\n",
    "disc_optimizer = keras.optimizers.Adam(learning_rate=disc_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_patch = 23\n",
    "# int(hr_height / 2 ** 4)\n",
    "\n",
    "width_patch = 40\n",
    "# int(hr_width / 2 ** 4)\n",
    "\n",
    "disc_patch = (height_patch, width_patch, 1)\n",
    "# disc_patch\n",
    "\n",
    "pretrain_iteration = 1\n",
    "train_iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pretrain_step(gen_model, x, y):\n",
    "    \"\"\"\n",
    "    Single step of generator pre-training.\n",
    "    Args:\n",
    "        gen_model: A compiled generator model.\n",
    "        x: The low resolution image tensor.\n",
    "        y: The high resolution image tensor.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_hr = gen_model(x)\n",
    "        loss_mse = tf.keras.losses.MeanSquaredError()(y, fake_hr)\n",
    "\n",
    "    grads = tape.gradient(loss_mse, gen_model.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(grads, gen_model.trainable_variables))\n",
    "\n",
    "    return loss_mse\n",
    "\n",
    "\n",
    "def pretrain_generator(gen_model, dataset, writer):\n",
    "    \"\"\"Function that pretrains the generator slightly, to avoid local minima.\n",
    "    Args:\n",
    "        gen_model: A compiled generator model.\n",
    "        dataset: A tf dataset object of low and high res images to pretrain over.\n",
    "        writer: A summary writer object.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        for _ in range(1):\n",
    "            for x, y in dataset:\n",
    "                loss = pretrain_step(gen_model, x, y)\n",
    "                if pretrain_iteration % 20 == 0:\n",
    "                    print(f'Pretrain Step: {pretrain_iteration}, Pretrain MSE Loss: {loss}')\n",
    "                    tf.summary.scalar('MSE Loss', loss, step=tf.cast(pretrain_iteration, tf.int64))\n",
    "                    writer.flush()\n",
    "                pretrain_iteration += 1\n",
    "\n",
    "@tf.function\n",
    "def train_step(gen_model, disc_model, x, y):\n",
    "    \"\"\"Single train step function for the SRGAN.\n",
    "    Args:\n",
    "        gen_model: A compiled generator model.\n",
    "        disc_model: A compiled discriminator model.\n",
    "        x: The low resolution input image.\n",
    "        y: The desired high resolution output image.\n",
    "    Returns:\n",
    "        disc_loss: The mean loss of the discriminator.\n",
    "        adv_loss: The Binary Crossentropy loss between real label and predicted label.\n",
    "        cont_loss: The Mean Square Error of VGG19 feature extracted original image (y) and VGG19 feature extractedgenerated image (y_hat).\n",
    "        mse_loss: The Mean Square Error of original image (y) and generated image (y_hat).\n",
    "    \"\"\"\n",
    "    # Label smoothing for better gradient flow\n",
    "    valid = tf.ones((x.shape[0],) + disc_patch)\n",
    "    fake = tf.zeros((x.shape[0],) + disc_patch)\n",
    "#     print('label')\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # From low res. image generate high res. version\n",
    "        fake_hr = gen_model(x)\n",
    "#         print('gen_model')\n",
    "\n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        valid_prediction = disc_model(y)\n",
    "        fake_prediction = disc_model(fake_hr)\n",
    "#         print('disc_model')\n",
    "        # Generator loss\n",
    "        cont_loss = content_loss(y, fake_hr)\n",
    "        adv_loss = 1e-3 * tf.keras.losses.BinaryCrossentropy()(valid, fake_prediction)\n",
    "        mse_loss = tf.keras.losses.MeanSquaredError()(y, fake_hr)\n",
    "        perceptual_loss = cont_loss + adv_loss + mse_loss\n",
    "\n",
    "        # Discriminator loss\n",
    "        valid_loss = tf.keras.losses.BinaryCrossentropy()(valid, valid_prediction)\n",
    "        fake_loss = tf.keras.losses.BinaryCrossentropy()(fake, fake_prediction)\n",
    "        disc_loss = tf.add(valid_loss, fake_loss)\n",
    "\n",
    "#         print('finish gradient')\n",
    "        \n",
    "    # Backprop on Generator\n",
    "    gen_grads = gen_tape.gradient(perceptual_loss, gen_model.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gen_grads, gen_model.trainable_variables))\n",
    "\n",
    "    # Backprop on Discriminator\n",
    "    disc_grads = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "    disc_optimizer.apply_gradients(zip(disc_grads, disc_model.trainable_variables))\n",
    "#     print('optimizer')\n",
    "    \n",
    "    return disc_loss, adv_loss, cont_loss, mse_loss\n",
    "\n",
    "\n",
    "def train(gen_model, disc_model, dataset, writer, log_iter=200):\n",
    "    \"\"\"\n",
    "    Function that defines a single training step for the SR-GAN.\n",
    "    Args:\n",
    "        gen_model: A compiled generator model.\n",
    "        disc_model: A compiled discriminator model.\n",
    "        dataset: A tf data object that contains low and high res images.\n",
    "        log_iter: Number of iterations after which to add logs in \n",
    "                  tensorboard.\n",
    "        writer: Summary writer\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        # Iterate over dataset\n",
    "        for x, y in dataset:\n",
    "            disc_loss, adv_loss, cont_loss, mse_loss = train_step(gen_model, disc_model, x, y)\n",
    "#             print(train_iteration)\n",
    "            # Log tensorboard summaries if log iteration is reached.\n",
    "            if train_iteration % log_iter == 0:\n",
    "                print(f'Train Step: {train_iteration}, Adversarial Loss: {adv_loss}, Content Loss: {cont_loss}, MSE Loss: {mse_loss}, Discriminator Loss: {disc_loss}')\n",
    "                \n",
    "                tf.summary.scalar('Adversarial Loss', adv_loss, step=train_iteration)\n",
    "                tf.summary.scalar('Content Loss', cont_loss, step=train_iteration)\n",
    "                tf.summary.scalar('MSE Loss', mse_loss, step=train_iteration)\n",
    "                tf.summary.scalar('Discriminator Loss', disc_loss, step=train_iteration)\n",
    "                tf.summary.image('Low Res', tf.cast(255 * x, tf.uint8), step=train_iteration)\n",
    "                tf.summary.image('High Res', tf.cast(255 * (y + 1.0) / 2.0, tf.uint8), step=train_iteration)\n",
    "                tf.summary.image('Generated', tf.cast(255 * (gen_model.predict(x) + 1.0) / 2.0, tf.uint8), step=train_iteration)\n",
    "                gen_model.save('models/generator.h5')\n",
    "                disc_model.save('models/discriminator.h5')\n",
    "                writer.flush()\n",
    "            train_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Step: 20, Pretrain MSE Loss: 0.2115798145532608\n",
      "Pretrain Step: 40, Pretrain MSE Loss: 0.11454479396343231\n",
      "Pretrain Step: 60, Pretrain MSE Loss: 0.06302903592586517\n",
      "Pretrain Step: 80, Pretrain MSE Loss: 0.014448248781263828\n",
      "Train Step: 20, Adversarial Loss: 0.001265631290152669, Content Loss: 0.19229301810264587, MSE Loss: 0.06551827490329742, Discriminator Loss: 0.9399911165237427\n",
      "Train Step: 40, Adversarial Loss: 0.0007144010160118341, Content Loss: 0.1103486493229866, MSE Loss: 0.013762152753770351, Discriminator Loss: 1.3704360723495483\n",
      "Train Step: 60, Adversarial Loss: 0.0006632150616496801, Content Loss: 0.17036882042884827, MSE Loss: 0.0493137426674366, Discriminator Loss: 1.1672842502593994\n",
      "Train Step: 80, Adversarial Loss: 0.0007247310131788254, Content Loss: 0.06420890986919403, MSE Loss: 0.010135100223124027, Discriminator Loss: 1.2994441986083984\n",
      "Train Step: 20, Adversarial Loss: 0.0009622846264392138, Content Loss: 0.1769925355911255, MSE Loss: 0.0484156496822834, Discriminator Loss: 0.964571475982666\n",
      "Train Step: 40, Adversarial Loss: 0.000837200612295419, Content Loss: 0.10470155626535416, MSE Loss: 0.011318909004330635, Discriminator Loss: 1.336801528930664\n",
      "Train Step: 60, Adversarial Loss: 0.0008562427829019725, Content Loss: 0.16531898081302643, MSE Loss: 0.04391906037926674, Discriminator Loss: 0.8534897565841675\n",
      "Train Step: 80, Adversarial Loss: 0.0011286867083981633, Content Loss: 0.06060698255896568, MSE Loss: 0.008816652931272984, Discriminator Loss: 1.351170301437378\n"
     ]
    }
   ],
   "source": [
    "# utilize the multiple GPUs\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "    \n",
    "with tf.device('/device:GPU:1'):\n",
    "    # Define the directory for saving pretrainig loss tensorboard summary.\n",
    "    pretrain_summary_writer = tf.summary.create_file_writer('logs/pretrain')\n",
    "\n",
    "    # Run pre-training.\n",
    "#     sample_train_dataset\n",
    "#     train_dataset\n",
    "    pretrain_generator(gen_model, train_dataset, pretrain_summary_writer)\n",
    "    gen_model.save('models/generator.h5')\n",
    "    \n",
    "    # Define the directory for saving the SRGAN training tensorbaord summary.\n",
    "    train_summary_writer = tf.summary.create_file_writer('logs/train')\n",
    "\n",
    "    epochs = 2\n",
    "\n",
    "    # Run training.\n",
    "    for _ in range(epochs):\n",
    "        print('===================')\n",
    "        print(f'Epoch: {_}\\n')\n",
    "        \n",
    "        train(gen_model, disc_model, train_dataset, train_summary_writer, log_iter=200)\n",
    "\n",
    "# ValueError: Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops. You can implement `tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch size like:\n",
    "# ```\n",
    "# with strategy.scope():\n",
    "#     loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "# ....\n",
    "#     loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'figisze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b49fae25d4be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigisze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_val_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\srv_gan\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\srv_gan\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[1;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \"\"\"\n\u001b[1;32m-> 1287\u001b[1;33m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfig_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[0;32m   1289\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\srv_gan\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m                                         \u001b[0mframeon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                                         \u001b[0mFigureClass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFigureClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfigLabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\srv_gan\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;34m\"\"\"Create a new figure manager instance.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_figure_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\srv_gan\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[1;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3491\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3492\u001b[0m         \u001b[0mfig_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FigureClass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3493\u001b[1;33m         \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3494\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_figure_manager_given_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'figisze'"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    # Recreate the exact same model, including its weights and the optimizer\n",
    "    gen_model = tf.keras.models.load_model('models/generator.h5')\n",
    "\n",
    "    # Show the model architecture\n",
    "#     gen_model.summary()\n",
    "    val_pred = gen_model.predict(sample_val_dataset)\n",
    "    val_pred = tf.cast(255 * (val_pred + 1.0) / 2.0, tf.uint8)\n",
    "\n",
    "for i in sample_val_dataset.take(1):\n",
    "    test_val = i\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(50, 25))\n",
    "ax1.imshow(val_pred[0])\n",
    "ax2.imshow(test_val[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12220), started 0:16:47 ago. (Use '!kill 12220' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c51cb6df94c3d8bd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c51cb6df94c3d8bd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs\n",
    "# !kill 16092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
